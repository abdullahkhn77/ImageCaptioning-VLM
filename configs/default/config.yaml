# Main training / experiment config
# Override via: python scripts/train.py training.batch_size=8 model.name=google/paligemma-3b

defaults:
  - model
  - dataset
  - lora

# Experiment
experiment_name: vlm_caption_default
seed: 42

# Training (merged with model/dataset/lora from defaults)
training:
  output_dir: outputs/checkpoints
  num_epochs: 3
  batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.03
  max_grad_norm: 1.0
  logging_steps: 10
  save_steps: 500
  eval_steps: 500
  save_total_limit: 3
  bf16: true
  fp16: false
  gradient_checkpointing: true
  dataloader_num_workers: 4
  report_to: mlflow  # or wandb, none

# Paths
paths:
  data_dir: data
  raw_data: data/raw
  processed_data: data/processed

# Checkpoint (for evaluate / export)
checkpoint_dir: null
