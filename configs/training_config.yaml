# Training Configuration
model:
  name: "Salesforce/blip-image-captioning-base"
  max_length: 50
  num_beams: 4

training:
  learning_rate: 1.0e-5
  batch_size: 16
  num_epochs: 10
  warmup_steps: 500
  weight_decay: 0.01
  gradient_accumulation_steps: 1
  fp16: true
  save_steps: 1000
  eval_steps: 500
  logging_steps: 100

data:
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  image_size: 224
  max_caption_length: 50

paths:
  data_dir: "data/processed"
  output_dir: "models/checkpoints"
  cache_dir: ".cache"

evaluation:
  metrics:
    - "bleu"
    - "rouge"
    - "cider"
  compute_on_train: false

mlflow:
  experiment_name: "vlm-captioning"
  tracking_uri: "./mlruns"
  log_model: true

