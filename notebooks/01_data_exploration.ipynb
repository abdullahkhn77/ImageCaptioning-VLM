{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 — Data Exploration\n",
    "\n",
    "Explore the captioning dataset: visualise samples, inspect caption statistics,\n",
    "and preview augmentation transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import yaml\n",
    "from PIL import Image\n",
    "\n",
    "# Make src/ importable\n",
    "sys.path.insert(0, str(Path(\"../src\").resolve()))\n",
    "\n",
    "# Load config\n",
    "with open(\"../configs/default.yaml\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "IMAGE_ROOT = Path(\"..\") / cfg[\"data\"][\"image_root\"]\n",
    "TRAIN_PATH = Path(\"..\") / cfg[\"data\"][\"train_annotations\"]\n",
    "\n",
    "print(f\"Image root : {IMAGE_ROOT}\")\n",
    "print(f\"Train file : {TRAIN_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotations(path):\n",
    "    \"\"\"Load from JSON or JSONL, flattening multi-caption entries.\"\"\"\n",
    "    path = Path(path)\n",
    "    if path.suffix == \".jsonl\":\n",
    "        raw = [json.loads(l) for l in open(path) if l.strip()]\n",
    "    else:\n",
    "        raw = json.load(open(path))\n",
    "    flat = []\n",
    "    for e in raw:\n",
    "        if \"captions\" in e:\n",
    "            for c in e[\"captions\"]:\n",
    "                flat.append({\"image\": e[\"image\"], \"caption\": c})\n",
    "        else:\n",
    "            flat.append(e)\n",
    "    return flat\n",
    "\n",
    "annotations = load_annotations(TRAIN_PATH)\n",
    "print(f\"Total caption entries: {len(annotations):,}\")\n",
    "unique_images = set(e[\"image\"] for e in annotations)\n",
    "print(f\"Unique images:         {len(unique_images):,}\")\n",
    "print(f\"Captions per image:    {len(annotations) / max(len(unique_images), 1):.1f}\")\n",
    "print()\n",
    "print(\"Sample entry:\")\n",
    "annotations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample images with captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 8 random samples\n",
    "rng = np.random.default_rng(42)\n",
    "indices = rng.choice(len(annotations), size=min(8, len(annotations)), replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 9))\n",
    "for ax, idx in zip(axes.flat, indices):\n",
    "    entry = annotations[idx]\n",
    "    img_path = IMAGE_ROOT / entry[\"image\"]\n",
    "    try:\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "    except Exception:\n",
    "        img = Image.new(\"RGB\", (224, 224), (200, 200, 200))\n",
    "    ax.imshow(img)\n",
    "    caption = entry[\"caption\"]\n",
    "    wrapped = \"\\n\".join([caption[i:i+40] for i in range(0, len(caption), 40)])\n",
    "    ax.set_title(wrapped, fontsize=8)\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Random training samples\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Caption length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_lengths = [len(e[\"caption\"]) for e in annotations]\n",
    "word_lengths = [len(e[\"caption\"].split()) for e in annotations]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.hist(char_lengths, bins=60, edgecolor=\"black\", alpha=0.7)\n",
    "ax1.set_xlabel(\"Caption length (characters)\")\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.set_title(\"Character-length distribution\")\n",
    "ax1.axvline(np.mean(char_lengths), color=\"red\", linestyle=\"--\",\n",
    "            label=f\"mean = {np.mean(char_lengths):.0f}\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.hist(word_lengths, bins=40, edgecolor=\"black\", alpha=0.7, color=\"orange\")\n",
    "ax2.set_xlabel(\"Caption length (words)\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.set_title(\"Word-length distribution\")\n",
    "ax2.axvline(np.mean(word_lengths), color=\"red\", linestyle=\"--\",\n",
    "            label=f\"mean = {np.mean(word_lengths):.1f}\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Characters — min: {min(char_lengths)}, max: {max(char_lengths)}, \"\n",
    "      f\"mean: {np.mean(char_lengths):.1f}, median: {np.median(char_lengths):.0f}\")\n",
    "print(f\"Words      — min: {min(word_lengths)}, max: {max(word_lengths)}, \"\n",
    "      f\"mean: {np.mean(word_lengths):.1f}, median: {np.median(word_lengths):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vocabulary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from all captions\n",
    "word_counter = Counter()\n",
    "for e in annotations:\n",
    "    tokens = e[\"caption\"].lower().split()\n",
    "    word_counter.update(tokens)\n",
    "\n",
    "vocab_size = len(word_counter)\n",
    "total_tokens = sum(word_counter.values())\n",
    "print(f\"Vocabulary size:  {vocab_size:,}\")\n",
    "print(f\"Total tokens:     {total_tokens:,}\")\n",
    "print(f\"Hapax legomena:   {sum(1 for c in word_counter.values() if c == 1):,} \"\n",
    "      f\"({sum(1 for c in word_counter.values() if c == 1) / vocab_size * 100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Top 30 most common words\n",
    "top30 = word_counter.most_common(30)\n",
    "words, counts = zip(*top30)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "ax.bar(words, counts, edgecolor=\"black\", alpha=0.7)\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Top 30 most common words\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency vs rank (Zipf's law)\n",
    "freqs = sorted(word_counter.values(), reverse=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.loglog(range(1, len(freqs) + 1), freqs, alpha=0.7)\n",
    "ax.set_xlabel(\"Rank\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Word frequency vs. rank (log-log)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Augmented image samples\n",
    "\n",
    "Preview the training-time augmentation pipeline defined in the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import build_train_transforms, build_eval_transforms\n",
    "\n",
    "aug_cfg = cfg[\"data\"].get(\"augmentation\", {})\n",
    "image_size = cfg[\"data\"][\"image_size\"]\n",
    "\n",
    "train_tfm = build_train_transforms(aug_cfg, image_size)\n",
    "eval_tfm = build_eval_transforms(image_size)\n",
    "\n",
    "print(\"Train transforms:\", train_tfm)\n",
    "print(\"Eval transforms: \", eval_tfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one image and show it with several augmentation rolls\n",
    "sample = annotations[0]\n",
    "img_path = IMAGE_ROOT / sample[\"image\"]\n",
    "try:\n",
    "    original = Image.open(img_path).convert(\"RGB\")\n",
    "except Exception:\n",
    "    original = Image.new(\"RGB\", (320, 240), (180, 180, 180))\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(18, 7))\n",
    "\n",
    "# Row 1: original + eval transform\n",
    "axes[0, 0].imshow(original)\n",
    "axes[0, 0].set_title(\"Original\", fontsize=9)\n",
    "axes[0, 0].axis(\"off\")\n",
    "\n",
    "eval_img = eval_tfm(original)\n",
    "axes[0, 1].imshow(eval_img)\n",
    "axes[0, 1].set_title(\"Eval transform\", fontsize=9)\n",
    "axes[0, 1].axis(\"off\")\n",
    "\n",
    "for ax in axes[0, 2:]:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Row 2: 5 different augmentation rolls\n",
    "for i in range(5):\n",
    "    aug_img = train_tfm(original)\n",
    "    axes[1, i].imshow(aug_img)\n",
    "    axes[1, i].set_title(f\"Aug #{i+1}\", fontsize=9)\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Augmentation preview — {sample['image']}\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
